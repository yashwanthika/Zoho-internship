{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "# day5: 26 Aug 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Loss Functions and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Demonstrate your understanding of MSE and Cross Entropy Loss by implementing them in Numpy\n",
    "2. Implement equivalent \"backward\" functions that would compute the derivative of the above loss functions in Numpy\n",
    "3. Verify the gradients computed by your function with that of PyTorch's autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(x, m, b):\n",
    "    yhat = x * m + b\n",
    "    return yhat\n",
    "\n",
    "\n",
    "def MSE(y, yhat):\n",
    "    return np.square(np.subtract(y, yhat)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_MSE_wrt_m(x, y, yhat):\n",
    "    sum = 0\n",
    "    for i in range(len(yhat)):\n",
    "        sum += -2 * x[i] * (y[i] - yhat[i])\n",
    "    return sum / len(y)\n",
    "\n",
    "\n",
    "def der_MSE_wrt_b(y, yhat):\n",
    "    sum = 0\n",
    "    for i in range(len(yhat)):\n",
    "        sum += -2 * (y[i] - yhat[i])\n",
    "    return sum / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: [ 3.4  9.  20.2  6.2 14.6 11.8]\n",
      "MSE: 0.3066666666666666\n",
      "gradient wrt m: 0.1333333333333274\n",
      "gradient wrt b: -0.2666666666666682\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0, 3.0, 7.0, 2.0, 5.0, 4.0])\n",
    "y = np.array([4.0, 9.0, 20.0, 7.0, 15.0, 11.0])\n",
    "m = 2.8\n",
    "b = 0.6\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)\n",
    "print(\"MSE:\", MSE(y, yhat))\n",
    "print(\"gradient wrt m:\", der_MSE_wrt_m(x, y, yhat))\n",
    "print(\"gradient wrt b:\", der_MSE_wrt_b(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: tensor([ 3.4000,  9.0000, 20.2000,  6.2000, 14.6000, 11.8000],\n",
      "       grad_fn=<AddBackward0>)\n",
      "MSE: tensor(0.3067, grad_fn=<MseLossBackward0>)\n",
      "gradient wrt m: tensor([0.1333])\n",
      "gradient wrt b: tensor([-0.2667])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 3.0, 7.0, 2.0, 5.0, 4.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 9.0, 20.0, 7.0, 15.0, 11.0])\n",
    "m = torch.tensor([2.8], requires_grad=True)\n",
    "b = torch.tensor([0.6], requires_grad=True)\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)\n",
    "mse = torch.nn.functional.mse_loss(linear_regression(x, m, b), y)\n",
    "print(\"MSE:\", mse)\n",
    "mse.backward()\n",
    "print(\"gradient wrt m:\", m.grad)\n",
    "print(\"gradient wrt b:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: [3.6]\n",
      "MSE: 0.15999999999999992\n",
      "gradient wrt m: -0.7999999999999998\n",
      "gradient wrt b: -0.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0])\n",
    "y = np.array([4.0])\n",
    "m = 3.0\n",
    "b = 0.6\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)\n",
    "print(\"MSE:\", MSE(y, yhat))\n",
    "print(\"gradient wrt m:\", der_MSE_wrt_m(x, y, yhat))\n",
    "print(\"gradient wrt b:\", der_MSE_wrt_b(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: tensor([3.6000], grad_fn=<AddBackward0>)\n",
      "MSE: tensor(0.1600, grad_fn=<MseLossBackward0>)\n",
      "gradient wrt m: tensor([-0.8000])\n",
      "gradient wrt b: tensor([-0.8000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([4.0])\n",
    "m = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([0.6], requires_grad=True)\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)\n",
    "mse = torch.nn.functional.mse_loss(linear_regression(x, m, b), y)\n",
    "print(\"MSE:\", mse)\n",
    "mse.backward()\n",
    "print(\"gradient wrt m:\", m.grad)\n",
    "print(\"gradient wrt b:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid_forward(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_backward(x):\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid * (1 - sigmoid)\n",
    "\n",
    "\n",
    "# cross entropy loss\n",
    "def BCE(yhat, y):\n",
    "    return -(y * np.log(yhat) + (1 - y) * np.log(1 - yhat)).mean()\n",
    "\n",
    "\n",
    "def BCE_wrt_z(y, yhat):\n",
    "    der_BCE = (yhat - y) / (yhat * (1 - yhat))\n",
    "    der_sigmoid = sigmoid_backward(yhat)\n",
    "    return der_BCE * der_sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: [0.11920292 0.73105858]\n",
      "Cross entropy loss: 0.22009484928059767\n",
      "Grad: [ 0.28282793 -0.30006058]\n"
     ]
    }
   ],
   "source": [
    "z = np.array([-2, 1])\n",
    "y = np.array([0, 1])\n",
    "sigmoid_forward_v = np.vectorize(sigmoid_forward)\n",
    "yhat = sigmoid_forward_v(z)\n",
    "print(\"predicted y:\", sigmoid_forward_v(z))\n",
    "loss = BCE(yhat, y)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "print(\"Grad:\", BCE_wrt_z(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: tensor([0.1192, 0.7311], grad_fn=<SigmoidBackward0>)\n",
      "Cross entropy loss: tensor(0.2201, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Grad: tensor([ 0.0596, -0.1345])\n",
      "<BinaryCrossEntropyBackward0 object at 0x7f131930e9e0>\n",
      "<SigmoidBackward0 object at 0x7f12e2463250>\n",
      "<AccumulateGrad object at 0x7f12e2463f40>\n",
      "Tensor with grad found: tensor([-2.,  1.], requires_grad=True)\n",
      " - gradient: tensor([ 0.0596, -0.1345])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([-2.0, 1.0], requires_grad=True)\n",
    "y = torch.tensor([0.0, 1.0], requires_grad=False)\n",
    "yhat = torch.sigmoid(z)\n",
    "print(\"predicted y:\", yhat)\n",
    "loss = torch.nn.BCELoss()(yhat, y)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "loss.backward()\n",
    "print(\"Grad:\", z.grad)\n",
    "\n",
    "\n",
    "def getBack(var_grad_fn):\n",
    "    print(var_grad_fn)\n",
    "    for fn in var_grad_fn.next_functions:\n",
    "        if fn[0]:\n",
    "            try:\n",
    "                tensor = getattr(fn[0], \"variable\")\n",
    "                print(fn[0])\n",
    "                print(\"Tensor with grad found:\", tensor)\n",
    "                print(\" - gradient:\", tensor.grad)\n",
    "                print()\n",
    "            except AttributeError as e:\n",
    "                getBack(fn[0])\n",
    "\n",
    "\n",
    "getBack(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted y: [5.55948233e-06 6.69285092e-03 9.99999985e-01 1.00000000e+00\n",
      " 9.99999959e-01]\n",
      "Cross entropy loss: 0.0013441929233032343\n",
      "Grad: [ 0.25000139  0.25168167 -0.19661194 -0.19661193 -0.19661195]\n"
     ]
    }
   ],
   "source": [
    "z = np.array([-12.1, -5.0, 18.0, 29.0, 17.0])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "yhat = sigmoid_forward(z)\n",
    "print(\"Predicted y:\", sigmoid_forward(z))\n",
    "loss = BCE(yhat, y)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "print(\"Grad:\", BCE_wrt_z(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted y: tensor([5.5595e-06, 6.6929e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "Cross entropy loss: tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Grad tensor([1.1119e-06, 1.3386e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([-12.1, -5.0, 18.0, 29.0, 17.0], requires_grad=True)\n",
    "y = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0], requires_grad=False)\n",
    "print(\"Predicted y:\", torch.sigmoid(z))\n",
    "loss = torch.nn.BCELoss()(torch.sigmoid(z), y)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "loss.backward()\n",
    "print(\"Grad\", z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd backward traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: tensor([ 8., 15.], grad_fn=<MulBackward0>)\n",
      "Grad wrt x: tensor([4., 3.])\n",
      "Grad wrt y: tensor([2., 5.])\n",
      "<MulBackward0 object at 0x7f132af36440>\n",
      "<AccumulateGrad object at 0x7f132aeffca0>\n",
      "Tensor with grad found: tensor([2., 5.], requires_grad=True)\n",
      " - gradient: tensor([4., 3.])\n",
      "\n",
      "<AccumulateGrad object at 0x7f13283f0460>\n",
      "Tensor with grad found: tensor([4., 3.], requires_grad=True)\n",
      " - gradient: tensor([2., 5.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0, 5.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 3.0], requires_grad=True)\n",
    "z = x * y\n",
    "print(\"Product:\", z)\n",
    "external_grad = torch.ones_like(x)\n",
    "z.backward(gradient=external_grad)\n",
    "print(\"Grad wrt x:\", x.grad)\n",
    "print(\"Grad wrt y:\", y.grad)\n",
    "\n",
    "\n",
    "def getBack(var_grad_fn):\n",
    "    print(var_grad_fn)\n",
    "    for fn in var_grad_fn.next_functions:\n",
    "        if fn[0]:\n",
    "            try:\n",
    "                tensor = getattr(fn[0], \"variable\")\n",
    "                print(fn[0])\n",
    "                print(\"Tensor with grad found:\", tensor)\n",
    "                print(\" - gradient:\", tensor.grad)\n",
    "                print()\n",
    "            except AttributeError as e:\n",
    "                getBack(fn[0])\n",
    "\n",
    "\n",
    "getBack(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why do we pass external gradients as no of ones as that of the size of tensor we backpropagate in backward()?\n",
    "\n",
    "Since we have to trace back each element of the tensor and dz/dz = 1 .Hence, we pass external gradients as no of ones as that of the size of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: tensor([ 8., 15.], grad_fn=<MulBackward0>)\n",
      "Grad wrt x: tensor([1.5435, 0.3765])\n",
      "Grad wrt y: tensor([0.7717, 0.6275])\n"
     ]
    }
   ],
   "source": [
    "# Example of random gradient outout being passed\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0, 5.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 3.0], requires_grad=True)\n",
    "z = x * y\n",
    "print(\"Product:\", z)\n",
    "external_grad = torch.rand_like(z)\n",
    "z.backward(gradient=external_grad)\n",
    "print(\"Grad wrt x:\", x.grad)\n",
    "print(\"Grad wrt y:\", y.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('study_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a24480c7c253f489f01188e56c2de50a3ffa81977a4b76b34cdd2b40b0691c29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
