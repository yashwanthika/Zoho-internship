{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "# day5: 26 Aug 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Loss Functions and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Demonstrate your understanding of MSE and Cross Entropy Loss by implementing them in Numpy\n",
    "2. Implement equivalent \"backward\" functions that would compute the derivative of the above loss functions in Numpy\n",
    "3. Verify the gradients computed by your function with that of PyTorch's autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Square Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#linear regression\n",
    "def linear_regression(x, m, b):\n",
    "    yhat = x * m + b\n",
    "    return yhat\n",
    "\n",
    "# Mean Square Error (MSE)\n",
    "def MSE(y, yhat):\n",
    "    return np.square(np.subtract(y, yhat)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Derivative of MSE\n",
    "\n",
    "def der_MSE_wrt_m(x, y, yhat):\n",
    "    sum = 0\n",
    "    for i in range(len(yhat)):\n",
    "        sum += -2 * x[i] * (y[i] - yhat[i])\n",
    "    return sum / len(y)\n",
    "\n",
    "\n",
    "def der_MSE_wrt_b(y, yhat):\n",
    "    sum = 0\n",
    "    for i in range(len(yhat)):\n",
    "        sum += -2 * (y[i] - yhat[i])\n",
    "    return sum / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: [ 3.4  9.  20.2  6.2 14.6 11.8]\n"
     ]
    }
   ],
   "source": [
    "# implementation of Mean Square Error using numpy\n",
    "x = np.array([1.0, 3.0, 7.0, 2.0, 5.0, 4.0])\n",
    "y = np.array([4.0, 9.0, 20.0, 7.0, 15.0, 11.0])\n",
    "m = 2.8\n",
    "b = 0.6\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: 0.3066666666666666\n",
      "gradient wrt m: 0.1333333333333274\n",
      "gradient wrt b: -0.2666666666666682\n"
     ]
    }
   ],
   "source": [
    "# Computing loss and back propagation\n",
    "print(\"MSE:\", MSE(y, yhat))\n",
    "print(\"gradient wrt m:\", der_MSE_wrt_m(x, y, yhat))\n",
    "print(\"gradient wrt b:\", der_MSE_wrt_b(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: tensor([ 3.4000,  9.0000, 20.2000,  6.2000, 14.6000, 11.8000],\n",
      "       grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# verification of MSE with torch and backpropagation using autograd\n",
    "x = torch.tensor([1.0, 3.0, 7.0, 2.0, 5.0, 4.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 9.0, 20.0, 7.0, 15.0, 11.0])\n",
    "m = torch.tensor([2.8], requires_grad=True)\n",
    "b = torch.tensor([0.6], requires_grad=True)\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE: tensor(0.3067, grad_fn=<MseLossBackward0>)\n",
      "gradient wrt m: tensor([0.1333])\n",
      "gradient wrt b: tensor([-0.2667])\n"
     ]
    }
   ],
   "source": [
    "# Computing loss and back propagation using autograd\n",
    "mse = torch.nn.functional.mse_loss(linear_regression(x, m, b), y)\n",
    "print(\"MSE:\", mse)\n",
    "mse.backward()\n",
    "print(\"gradient wrt m:\", m.grad)\n",
    "print(\"gradient wrt b:\", b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid_forward(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# derivative of sigmoid\n",
    "def sigmoid_backward(x):\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid * (1 - sigmoid)\n",
    "\n",
    "\n",
    "# Binary cross entropy loss (BCE)\n",
    "def BCE(yhat, y):\n",
    "    return -(y * np.log(yhat) + (1 - y) * np.log(1 - yhat)).mean()\n",
    "\n",
    "# derivative of BCE #dow(BCE)/dow(z)\n",
    "def der_BCE_wrt_z(y, yhat):\n",
    "    der_BCE = -(y/yhat -(1-y)/(1-yhat)) #dow(BCE)/dow(yhat)\n",
    "    der_sigmoid = sigmoid_backward(yhat) #dow(yhat)/dow(z)\n",
    "    return  der_sigmoid *der_BCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted y: [5.55948233e-06 6.69285092e-03 9.99999985e-01 1.00000000e+00\n",
      " 9.99999959e-01]\n"
     ]
    }
   ],
   "source": [
    "# implementation of Sigmoid fn\n",
    "z = np.array([-12.1, -5.0, 18.0, 29.0, 17.0])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "yhat = sigmoid_forward(z)\n",
    "print(\"Predicted y:\", sigmoid_forward(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss: 0.0013441929233032343\n",
      "Grad: [ 0.25000139  0.25168167 -0.19661194 -0.19661193 -0.19661195]\n"
     ]
    }
   ],
   "source": [
    "# computation of loss and backpropagation\n",
    "loss = BCE(yhat, y)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "print(\"Grad:\", der_BCE_wrt_z(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted y: tensor([5.5595e-06, 6.6929e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "       grad_fn=<SigmoidBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# verification of BCE with torch and backpropagation using autograd\n",
    "z = torch.tensor([-12.1, -5.0, 18.0, 29.0, 17.0], requires_grad=True)\n",
    "y = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0], requires_grad=False)\n",
    "print(\"Predicted y:\", torch.sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss: tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Grad tensor([1.1119e-06, 1.3386e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "# computation of loss and backpropagation using autograd\n",
    "loss = torch.nn.BCELoss()(torch.sigmoid(z), y)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "loss.backward()\n",
    "print(\"Grad\", z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd backward traversal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: tensor([ 8., 15.], grad_fn=<MulBackward0>)\n",
      "Grad wrt x: tensor([4., 3.])\n",
      "Grad wrt y: tensor([2., 5.])\n",
      "<MulBackward0 object at 0x7fdee1612da0>\n",
      "<AccumulateGrad object at 0x7fdee1612b60>\n",
      "Tensor with grad found: tensor([2., 5.], requires_grad=True)\n",
      " - gradient: tensor([4., 3.])\n",
      "\n",
      "<AccumulateGrad object at 0x7fdee1613070>\n",
      "Tensor with grad found: tensor([4., 3.], requires_grad=True)\n",
      " - gradient: tensor([2., 5.])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0, 5.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 3.0], requires_grad=True)\n",
    "z = x * y\n",
    "print(\"Product:\", z)\n",
    "external_grad = torch.ones_like(x)\n",
    "z.backward(gradient=external_grad)\n",
    "print(\"Grad wrt x:\", x.grad)\n",
    "print(\"Grad wrt y:\", y.grad)\n",
    "\n",
    "\n",
    "def getBack(var_grad_fn):\n",
    "    print(var_grad_fn)\n",
    "    for fn in var_grad_fn.next_functions:\n",
    "        if fn[0]:\n",
    "            try:\n",
    "                tensor = getattr(fn[0], \"variable\")\n",
    "                print(fn[0])\n",
    "                print(\"Tensor with grad found:\", tensor)\n",
    "                print(\" - gradient:\", tensor.grad)\n",
    "                print()\n",
    "            except AttributeError as e:\n",
    "                getBack(fn[0])\n",
    "\n",
    "\n",
    "getBack(z.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why do we pass external gradients as no of ones as that of the size of tensor we backpropagate in backward()?\n",
    "\n",
    "Since we have to trace back each element of the tensor and dz/dz = 1 it can be replaced by weights.Hence, we pass external gradients as no of ones as that of the size of tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: tensor([ 8., 15.], grad_fn=<MulBackward0>)\n",
      "Grad wrt x: tensor([1.9012, 0.0511])\n",
      "Grad wrt y: tensor([0.9506, 0.0852])\n"
     ]
    }
   ],
   "source": [
    "# Example of random gradient outout being passed\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0, 5.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 3.0], requires_grad=True)\n",
    "z = x * y\n",
    "print(\"Product:\", z)\n",
    "external_grad = torch.rand_like(z)\n",
    "z.backward(gradient=external_grad)\n",
    "print(\"Grad wrt x:\", x.grad)\n",
    "print(\"Grad wrt y:\", y.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('study_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a24480c7c253f489f01188e56c2de50a3ffa81977a4b76b34cdd2b40b0691c29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
