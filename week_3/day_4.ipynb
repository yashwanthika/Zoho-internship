{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "# day4: 25 Aug 2022\n",
    "###  [Work in progress]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Loss Functions and Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Demonstrate your understanding of MSE and Cross Entropy Loss by implementing them in Numpy\n",
    "2. Implement equivalent \"backward\" functions that would compute the derivative of the above loss functions in Numpy\n",
    "3. Verify the gradients computed by your function with that of PyTorch's autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(x, m, b):\n",
    "    yhat = x * m + b\n",
    "    return yhat\n",
    "\n",
    "\n",
    "def MSE(y, yhat):\n",
    "    return np.square(np.subtract(y, yhat)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def der_MSE_wrt_m(x, y, yhat):\n",
    "    for i in range(len(yhat)):\n",
    "        sum = -2 * x[i] * (y[i] - yhat[i])\n",
    "    return sum / len(y)\n",
    "\n",
    "\n",
    "def der_MSE_wrt_b(y, yhat):\n",
    "    for i in range(len(yhat)):\n",
    "        sum = -2 * (y[i] - yhat[i])\n",
    "    return sum / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: [ 3.4  9.  20.2  6.2 14.6 11.8]\n",
      "MSE: 0.3066666666666666\n",
      "gradient wrt m: 1.0666666666666653\n",
      "gradient wrt b: 0.26666666666666633\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0, 3.0, 7.0, 2.0, 5.0, 4.0])\n",
    "y = np.array([4.0, 9.0, 20.0, 7.0, 15.0, 11.0])\n",
    "m = 2.8\n",
    "b = 0.6\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)\n",
    "print(\"MSE:\", MSE(y, yhat))\n",
    "print(\"gradient wrt m:\", der_MSE_wrt_m(x, y, yhat))\n",
    "print(\"gradient wrt b:\", der_MSE_wrt_b(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: tensor([ 3.4000,  9.0000, 20.2000,  6.2000, 14.6000, 11.8000],\n",
      "       grad_fn=<AddBackward0>)\n",
      "MSE: tensor(0.3067, grad_fn=<MseLossBackward0>)\n",
      "gradient wrt m: tensor([0.1333])\n",
      "gradient wrt b: tensor([-0.2667])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 3.0, 7.0, 2.0, 5.0, 4.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 9.0, 20.0, 7.0, 15.0, 11.0])\n",
    "m = torch.tensor([2.8], requires_grad=True)\n",
    "b = torch.tensor([0.6], requires_grad=True)\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)\n",
    "mse = torch.nn.functional.mse_loss(linear_regression(x, m, b), y)\n",
    "print(\"MSE:\", mse)\n",
    "mse.backward()\n",
    "print(\"gradient wrt m:\", m.grad)\n",
    "print(\"gradient wrt b:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: [3.6]\n",
      "MSE: 0.15999999999999992\n",
      "gradient wrt m: -0.7999999999999998\n",
      "gradient wrt b: -0.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1.0])\n",
    "y = np.array([4.0])\n",
    "m = 3.0\n",
    "b = 0.6\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)\n",
    "print(\"MSE:\", MSE(y, yhat))\n",
    "print(\"gradient wrt m:\", der_MSE_wrt_m(x, y, yhat))\n",
    "print(\"gradient wrt b:\", der_MSE_wrt_b(y, yhat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: tensor([3.6000], grad_fn=<AddBackward0>)\n",
      "MSE: tensor(0.1600, grad_fn=<MseLossBackward0>)\n",
      "gradient wrt m: tensor([-0.8000])\n",
      "gradient wrt b: tensor([-0.8000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = torch.tensor([4.0])\n",
    "m = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([0.6], requires_grad=True)\n",
    "yhat = linear_regression(x, m, b)\n",
    "print(\"predicted y:\", yhat)\n",
    "mse = torch.nn.functional.mse_loss(linear_regression(x, m, b), y)\n",
    "print(\"MSE:\", mse)\n",
    "mse.backward()\n",
    "print(\"gradient wrt m:\", m.grad)\n",
    "print(\"gradient wrt b:\", b.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid function\n",
    "def sigmoid_forward(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def sigmoid_backward(x):\n",
    "    sigmoid = 1 / (1 + np.exp(-x))\n",
    "    return sigmoid * (1 - sigmoid)\n",
    "\n",
    "\n",
    "# cross entropy loss\n",
    "def cross_entropy_loss(yHat, y):\n",
    "    return -np.sum(y * np.log(yHat)) / len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: [5.60279641e-09 9.99999994e-01]\n",
      "Cross entropy loss: 2.801398232353479e-09\n"
     ]
    }
   ],
   "source": [
    "z = np.array([-19, 19])\n",
    "y = np.array([0, 1])\n",
    "yhat = sigmoid_forward(z)\n",
    "print(\"predicted y:\", sigmoid_forward(z))\n",
    "loss = cross_entropy_loss(yhat, y)\n",
    "print(\"Cross entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y: tensor([5.6028e-09, 1.0000e+00], grad_fn=<SigmoidBackward0>)\n",
      "Cross entropy loss: tensor(0., grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Grad: tensor([2.8014e-09, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([-19.0, 19.0], requires_grad=True)\n",
    "y = torch.tensor([0.0, 1.0], requires_grad=False)\n",
    "print(\"predicted y:\", torch.sigmoid(z))\n",
    "loss = torch.nn.BCELoss()(torch.sigmoid(z), y)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "loss.backward()\n",
    "print(\"Grad:\", z.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted y: [5.55948233e-06 6.69285092e-03 9.99999985e-01 1.00000000e+00\n",
      " 9.99999959e-01]\n",
      "Cross entropy loss: 1.1325922106028611e-08\n"
     ]
    }
   ],
   "source": [
    "z = np.array([-12.1, -5.0, 18.0, 29.0, 17.0])\n",
    "y = np.array([0, 0, 1, 1, 1])\n",
    "yhat = sigmoid_forward(z)\n",
    "print(\"Predicted y:\", sigmoid_forward(z))\n",
    "loss = cross_entropy_loss(yhat, y)\n",
    "print(\"Cross entropy loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted y: tensor([5.5595e-06, 6.6929e-03, 1.0000e+00, 1.0000e+00, 1.0000e+00],\n",
      "       grad_fn=<SigmoidBackward0>)\n",
      "Cross entropy loss: tensor(0.0013, grad_fn=<BinaryCrossEntropyBackward0>)\n",
      "Grad tensor([1.1119e-06, 1.3386e-03, 0.0000e+00, 0.0000e+00, 0.0000e+00])\n"
     ]
    }
   ],
   "source": [
    "z = torch.tensor([-12.1, -5.0, 18.0, 29.0, 17.0], requires_grad=True)\n",
    "y = torch.tensor([0.0, 0.0, 1.0, 1.0, 1.0], requires_grad=False)\n",
    "print(\"Predicted y:\", torch.sigmoid(z))\n",
    "loss = torch.nn.BCELoss()(torch.sigmoid(z), y)\n",
    "print(\"Cross entropy loss:\", loss)\n",
    "loss.backward()\n",
    "print(\"Grad\", z.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('study_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a24480c7c253f489f01188e56c2de50a3ffa81977a4b76b34cdd2b40b0691c29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
