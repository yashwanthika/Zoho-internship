{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# week 3\n",
        "# Day3: 24 Aug 2022\n",
        "## Activation functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "1. Why is a non-linear activation function important? In particular, the underlying function being learnt by the ML algorithm may totally be unrelated to the activation function being used. So how is this non-linearity going to help? \n",
        "\n",
        "    Without non linear activation function the neural network will act as a linear function and can only be used for linearly seperable data. Non linear activation function brings in non linearity that is needed to seperate data that are not linearly seperable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. Demonstrate your understanding of different activation functions (atleast 5) by implementing them in Numpy\n",
        "3. Implement equivalent \"backward\" functions that would compute the derivative of the activation functions you've implemented above in Numpy\n",
        "4. Verify the gradients computed by your function with that of PyTorch's autograd "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "LOHHMQ9JzeL9"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Activation functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CX_bCTV7kMko",
        "outputId": "bacf1b67-95c1-49e2-cbc3-59ad1079dc6c"
      },
      "outputs": [],
      "source": [
        "#sigmoid function\n",
        "def sigmoid_forward(x):\n",
        "  return  1/(1+np.exp(-x))\n",
        "def sigmoid_backward(x):\n",
        "  sigmoid=1/(1+np.exp(-x))\n",
        "  return sigmoid*(1-sigmoid)\n",
        "#tanh function\n",
        "def tanh_forward(x):\n",
        "  return np.tanh(x)\n",
        "def tanh_backward(x):\n",
        "  return 1 - np.tanh(x)**2\n",
        "\n",
        "#Relu function\n",
        "def relu_forward(x):\n",
        "  return max(0,x)\n",
        "def relu_backward(x):\n",
        "  return 0 if x<=0 else 1 \n",
        "\n",
        "#Leaky relu function\n",
        "def leaky_relu_forward(x):\n",
        "  return max(0.01*x,x)\n",
        "def leaky_relu_backward(x):\n",
        "  return 0.01 if x<=0 else 1 \n",
        "\n",
        "#elu function\n",
        "def elu_forward(x,alpha=1):\n",
        "  return  x if x > 0 else alpha * (np.exp(x) - 1)\n",
        "def elu_backward(x):\n",
        "    if x > 0 :\n",
        "        return 1\n",
        "    else :\n",
        "        return np.exp(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Sigmoid "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid :  [0.00669285 0.5        0.88079708 0.95257413 0.99330715]\n",
            "Sigmoid backward: [0.00664806 0.25       0.10499359 0.04517666 0.00664806]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([-5.,0.,2.,3.,5.])\n",
        "print(\"Sigmoid : \",sigmoid_forward(x))\n",
        "print(\"Sigmoid backward:\",sigmoid_backward(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sigmoid: tensor([0.0067, 0.5000, 0.8808, 0.9526, 0.9933], grad_fn=<SigmoidBackward0>)\n",
            "Sigmoid backward: tensor([0.0066, 0.2500, 0.1050, 0.0452, 0.0066])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.tensor([-5.,0.,2.,3.,5.], requires_grad=True)\n",
        "y = torch.sigmoid(x)\n",
        "print(\"Sigmoid:\",y)\n",
        "external_grad = torch.ones_like(x)\n",
        "y.backward(gradient=external_grad)\n",
        "print(\"Sigmoid backward:\",x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Tanh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "BHYsOAfx3QAr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tanh :  [-0.9999092   0.          0.96402758  0.99505475  0.9999092 ]\n",
            "Tanh backward:  [1.81583231e-04 1.00000000e+00 7.06508249e-02 9.86603717e-03\n",
            " 1.81583231e-04]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([-5.,0.,2.,3.,5.])\n",
        "print(\"Tanh : \",tanh_forward(x))\n",
        "print(\"Tanh backward: \",tanh_backward(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tanh: tensor([-0.9999,  0.0000,  0.9640,  0.9951,  0.9999], grad_fn=<TanhBackward0>)\n",
            "Tanh backward: tensor([1.8155e-04, 1.0000e+00, 7.0651e-02, 9.8660e-03, 1.8155e-04])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.tensor([-5.,0.,2.,3.,5.], requires_grad=True)\n",
        "y = torch.tanh(x)\n",
        "print(\"Tanh:\",y)\n",
        "external_grad = torch.ones_like(x)\n",
        "y.backward(gradient=external_grad)\n",
        "print(\"Tanh backward:\",x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Relu :  [0 0 2 3 5]\n",
            "Relu backward: [0 0 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([-5.,0.,2.,3.,5.])\n",
        "relu_forward_v = np.vectorize(relu_forward)\n",
        "print(\"Relu : \",relu_forward_v(x))\n",
        "relu_backward_v= np.vectorize(relu_backward)\n",
        "print(\"Relu backward:\",relu_backward_v(x))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "relu: tensor([0., 0., 2., 3., 5.], grad_fn=<ReluBackward0>)\n",
            "relu backward: tensor([0., 0., 1., 1., 1.])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([-5.,0.,2.,3.,5.], requires_grad=True)\n",
        "y = torch.nn.functional.relu(x)\n",
        "print(\"relu:\",y)\n",
        "external_grad = torch.ones_like(x)\n",
        "y.backward(gradient=external_grad)\n",
        "print(\"relu backward:\",x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Leaky Relu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leaky Relu: [-0.05  0.    2.    3.    5.  ]\n",
            "Leaky Relu backward: [0.01 0.01 1.   1.   1.  ]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([-5.,0.,2.,3.,5.])\n",
        "leaky_relu_forward_v = np.vectorize(leaky_relu_forward)\n",
        "print(\"Leaky Relu:\",leaky_relu_forward_v(x))\n",
        "leaky_relu_backward_v = np.vectorize(leaky_relu_backward)\n",
        "print(\"Leaky Relu backward:\",leaky_relu_backward_v(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Leaky Relu: tensor([-0.0500,  0.0000,  2.0000,  3.0000,  5.0000],\n",
            "       grad_fn=<LeakyReluBackward0>)\n",
            "Leaky Relu backward: tensor([0.0100, 0.0100, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor([-5.,0.,2.,3.,5.], requires_grad=True)\n",
        "y = torch.nn.functional.leaky_relu(x)\n",
        "print(\"Leaky Relu:\",y)\n",
        "external_grad = torch.ones_like(x)\n",
        "y.backward(gradient=external_grad)\n",
        "print(\"Leaky Relu backward:\",x.grad)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Elu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Elu forward: [-0.99326205  0.          2.          3.          5.        ]\n",
            "Elu backward: [0.00673795 1.         1.         1.         1.        ]\n"
          ]
        }
      ],
      "source": [
        "x = np.array([-5.,0.,2.,3.,5.])\n",
        "elu_forward_v = np.vectorize(elu_forward)\n",
        "print(\"Elu forward:\",elu_forward_v(x))\n",
        "elu_backward_v = np.vectorize(elu_backward)\n",
        "print(\"Elu backward:\",elu_backward_v(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ELU: tensor([-0.9933,  0.0000,  2.0000,  3.0000,  5.0000], grad_fn=<EluBackward0>)\n",
            "ELU backward: tensor([0.0067, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.tensor([-5.,0.,2.,3.,5.], requires_grad=True)\n",
        "y = torch.nn.functional.elu(x)\n",
        "print(\"ELU:\",y)\n",
        "external_grad = torch.ones_like(x)\n",
        "y.backward(gradient=external_grad)\n",
        "print(\"ELU backward:\",x.grad)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "day_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.4 ('study_env')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "a24480c7c253f489f01188e56c2de50a3ffa81977a4b76b34cdd2b40b0691c29"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
