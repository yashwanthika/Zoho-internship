{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "day_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Week 3 \n",
        "# Day2 :Aug 23\n",
        "## 1. Write down a few functions (linear, quadratic, etc.) and compute their partial derivatives by hand. You can assume the input to be scalar values\n",
        "## 2. Write Python code to compute the gradients for the same functions with Numpy and verify if the results match with the ones you computed manually "
      ],
      "metadata": {
        "id": "FFZBCLIkZ6WZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "4u3a5dylRsDH"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QMGffr2SPuz9"
      },
      "cell_type": "code",
      "source": [
        "def forward_propagation(x):\n",
        "\n",
        "    A =  x ** 2\n",
        "    B = 2*x\n",
        "    C = A + B + 1\n",
        " \n",
        "    \n",
        "    return C"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e6W35I8-Pu0A",
        "outputId": "81ab68d8-e489-42cb-ca7d-57470a0942cb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "x = 3\n",
        "J = forward_propagation(x)\n",
        "print (\"J = \" + str(J))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "J = 16\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4O82ot95Pu0H"
      },
      "cell_type": "code",
      "source": [
        "def backward_propagation(x):\n",
        "\n",
        "    grad = 2 * x + 2\n",
        "    \n",
        "    return grad"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "_Gg9sNa5Pu0K",
        "outputId": "72bb53b5-81af-40d1-f741-d5fcf8b60e4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "x = 3\n",
        "grad = backward_propagation(x)\n",
        "print (\"grad= \" + str(grad))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grad= 8\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "pHx486ARPu0P"
      },
      "cell_type": "code",
      "source": [
        "def gradient_check(x, epsilon = 1e-7):\n",
        "    J_plus = forward_propagation(x+epsilon)      \n",
        "    J_minus = forward_propagation(x-epsilon)     \n",
        "    gradapprox = (J_plus - J_minus) / (2. * epsilon) \n",
        "    grad = backward_propagation(x)\n",
        "    print(grad,gradapprox)\n",
        "    numerator = np.linalg.norm(grad - gradapprox)                    \n",
        "    denominator = np.linalg.norm(grad) + np.linalg.norm(gradapprox)  \n",
        "    difference = numerator / denominator                             \n",
        "\n",
        "    if difference < 1e-7:\n",
        "        print (\"The gradient is correct!\")\n",
        "    else:\n",
        "        print (\"The gradient is wrong!\")\n",
        "    \n",
        "    return difference"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "id": "LLjrd7mhPu0S",
        "outputId": "25bf297f-5355-469e-9785-fdaa3f0ea245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "cell_type": "code",
      "source": [
        "x = 2\n",
        "difference = gradient_check(x)\n",
        "print(\"difference = \" + str(difference))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "The gradient is correct!\n",
            "difference = 8.182894368803594e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy \n",
        "x = numpy.array([2.,3.])\n",
        "difference = gradient_check(x)\n",
        "print(\"difference = \" + str(difference))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUpPHUwnvKLz",
        "outputId": "7857a092-b4ca-4c31-8218-ca4ff904f13d"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6. 8.] [5.99999999 7.99999999]\n",
            "The gradient is correct!\n",
            "difference = 8.182894368803594e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy \n",
        "x = numpy.array([2.,3.])\n",
        "difference = gradient_check(x)\n",
        "print(\"difference = \" + str(difference))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_ukkNues04Z",
        "outputId": "51380d99-16e0-4484-8763-1c40971e7131"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[6. 8.] [5.99999999 7.99999999]\n",
            "The gradient is correct!\n",
            "difference = 8.182894368803594e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Write Python code to compute the gradients for the same functions with PyTorch tensors without using PyTorch's autograd. Verify if the results match with the above ones"
      ],
      "metadata": {
        "id": "1mtAKVgEwpPZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor(2.0)\n",
        "grad=backward_propagation(x)\n",
        "print(grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHGkfPFqtk_p",
        "outputId": "ede9f4ca-87f2-4cb5-ea2b-6bd0f611885c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(6.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor(([2., 3.],[1.,3.]))\n",
        "grad=backward_propagation(x)\n",
        "print(grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UxgXGoAcva52",
        "outputId": "95b1b13a-852d-44af-a862-5a37ef3329ef"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6., 8.],\n",
            "        [4., 8.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Use PyTorch autograd to compute the gradients and verify if the results match with the ones from above"
      ],
      "metadata": {
        "id": "xtSxsJirwwWX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor(2.0, requires_grad = True)\n",
        "y = x**2 + 2*x + 1\n",
        "y.backward()\n",
        "dx = x.grad\n",
        "print(\"x.grad :\", int(dx))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1CtK25rvasR",
        "outputId": "542039cb-79b9-4017-e643-f2fc2535bbd3"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x.grad : 6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Do the same for vector/tensor inputs. Note down the shape of the results at each step and provide proper reasoning for the same"
      ],
      "metadata": {
        "id": "0muFFcG6w2FV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For vector values:"
      ],
      "metadata": {
        "id": "9IR9FZR4DRD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.tensor([2., 3.], requires_grad=True)\n",
        "y = x**2 + 2*x + 1\n",
        "external_grad = torch.ones_like(x)\n",
        "y.backward(gradient=external_grad)\n",
        "print(x.grad)\n",
        "print(x.grad.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWwj0WVlEFHi",
        "outputId": "f7832b31-b132-4356-b077-020107e9b542"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([6., 8.])\n",
            "torch.Size([2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For tensor values:"
      ],
      "metadata": {
        "id": "G2-rGaO7IgDT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor(([2., 3.],[1.,3.]), requires_grad=True)\n",
        "y = x**2 + 2*x + 1\n",
        "external_grad = torch.ones_like(x)\n",
        "y.backward(gradient=external_grad)\n",
        "print(x.grad)\n",
        "print(x.grad.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQqBa8vyIIWi",
        "outputId": "f281091e-2395-4bd0-aa2c-1b70a5058a26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[6., 8.],\n",
            "        [4., 8.]])\n",
            "torch.Size([2, 2])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. The CS231n lecture has a slide on \"Patterns in backward flow\" explaining about interesting properties of the gradients of specific functions. Demonstrate those properties with simple code snippets.\n",
        "\n",
        "Add gate:\n",
        "\n",
        "gradient distributor - The add gate always takes the gradient on its output and distributes it equally to all of its inputs, regardless of what their values were during the forward pass.\n",
        "\n",
        "max gate:  \n",
        "gradient router - the max gate distributes the gradient (unchanged) to exactly one of its inputs (the input that had the highest value during the forward pass).\n",
        "\n",
        "mul gate: \n",
        "\n",
        "gradient switcher -  Its gradients are the switched input values  multiplied by the gradient on its output during the chain rule."
      ],
      "metadata": {
        "id": "0C0c1lNID4p_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiplyGate(object):\n",
        "  def forward(self,x,y):\n",
        "    z=x*y\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "    return z\n",
        "  def backward(self,dz):\n",
        "    dx=self.y*dz\n",
        "    dy=self.x*dz\n",
        "    return [dx,dy]\n",
        "\n",
        "class AddGate(object):\n",
        "  def forward(self,x,y):\n",
        "    z=x+y\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "    return z\n",
        "  def backward(self,dz):\n",
        "    dx=dz\n",
        "    dy=dz\n",
        "    return [dx,dy]\n",
        "\n",
        "class MaxGate(object):\n",
        "  def forward(self,x,y):\n",
        "    z=max(x,y)\n",
        "    self.x=x\n",
        "    self.y=y\n",
        "    return z\n",
        "  def backward(self,dz):\n",
        "    if x>y:\n",
        "      dx=dz\n",
        "      dy=0.0\n",
        "    else:\n",
        "      dx=0.0\n",
        "      dy=dz\n",
        "    return [dx,dy]"
      ],
      "metadata": {
        "id": "goop6FgDJ1yq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x=-2.\n",
        "y=5.\n",
        "a=-4.\n",
        "b=3.\n",
        "add=AddGate()\n",
        "multiply=MultiplyGate()\n",
        "maximum=MaxGate()\n",
        "c=add.forward(x,y)\n",
        "print(\"c:\",c)\n",
        "e =maximum.forward(a,b)\n",
        "print(\"e:\",e)\n",
        "d=multiply.forward(c,e)\n",
        "print(\"d:\",d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rG1E6ZMPTpUY",
        "outputId": "fdbb38ed-a781-412c-ea3d-9b79c6c37add"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "c: 3.0\n",
            "e: 3.0\n",
            "d: 9.0\n"
          ]
        }
      ]
    }
  ]
}