{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "# day3: 31 Aug 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Learn about Gradient Descent and its below variants:\n",
    "\n",
    "    Momentum\n",
    "\n",
    "    Nesterov\n",
    "\n",
    "    Adagrad\n",
    "\n",
    "    RMSProp\n",
    "\n",
    "    Adam\n",
    "5. Implement all the above in Numpy\n",
    "6. How does the \"Exponential weighted average\" lecture given in the readings   relate to some of the variants of Gradient Descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 3.0, 7.0, 2.0, 5.0, 4.0])\n",
    "y = np.array([4.0, 9.0, 20.0, 7.0, 15.0, 11.0])\n",
    "\n",
    "\n",
    "def linear_regression(x, m, b):\n",
    "    yhat = x * m + b\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(\n",
    "    [\n",
    "        1.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        3.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        7.0,\n",
    "        0.0,\n",
    "        2.0,\n",
    "        5.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        4.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "    ]\n",
    ")\n",
    "y = np.array(\n",
    "    [\n",
    "        4.0,\n",
    "        0.0,\n",
    "        -1.0,\n",
    "        9.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        20.0,\n",
    "        0.0,\n",
    "        7.0,\n",
    "        15.0,\n",
    "        0.0,\n",
    "        0.1,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        11.0,\n",
    "        0.0,\n",
    "        1.0,\n",
    "        0.0,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.array(\n",
    "    [\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        1.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        3.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        7.0,\n",
    "        0.0,\n",
    "        2.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        5.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        4.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "    ]\n",
    ")\n",
    "y = np.array(\n",
    "    [\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        4.0,\n",
    "        0.0,\n",
    "        -1.0,\n",
    "        9.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        20.0,\n",
    "        0.0,\n",
    "        7.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        15.0,\n",
    "        0.0,\n",
    "        0.1,\n",
    "        0.0,\n",
    "        0.0,\n",
    "        11.0,\n",
    "        0.0,\n",
    "        1.0,\n",
    "        0.0,\n",
    "    ]\n",
    ")\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSProp\n",
    "class RMSProp:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.1\n",
    "        self.w = 9\n",
    "        self.b = 0\n",
    "        self.update_w = 0\n",
    "        self.update_b = 0\n",
    "        self.epsilon = 1e-6\n",
    "        self.beta = 0.9\n",
    "\n",
    "    def update_weight(self, x, y):\n",
    "        yhat = self.w * x + self.b\n",
    "        self.update_w = self.beta * self.update_w + (1 - self.beta) * (\n",
    "            (-2 * sum(x * (y - yhat)).mean()) ** 2\n",
    "        )\n",
    "        self.update_b = self.beta * self.update_b + (1 - self.beta) * (\n",
    "            (-2 * sum(y - yhat).mean()) ** 2\n",
    "        )\n",
    "        self.w -= (self.learning_rate / np.sqrt(self.update_w + self.epsilon)) * (\n",
    "            -2 * sum(x * (y - yhat)).mean()\n",
    "        )\n",
    "        self.b -= (self.learning_rate / np.sqrt(self.update_b + self.epsilon)) * (\n",
    "            -2 * sum(y - yhat).mean()\n",
    "        )\n",
    "\n",
    "    def MSE(self, y, yhat):\n",
    "        return np.square(np.subtract(y, yhat)).mean()\n",
    "\n",
    "    def fit(self, x, y, epochs=2000):\n",
    "        history = []\n",
    "        for e in range(epochs):\n",
    "            self.update_weight(x, y)\n",
    "            loss = self.MSE(y, (self.w * x + self.b))\n",
    "            if e % 100 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "            history.append(loss)\n",
    "            if loss <= 1:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "                return history\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 96.6050634024521\n",
      "weight:8.683772233984152,bias:-0.31622776599411734\n",
      "Epoch: 63, Loss: 0.8798530692190021\n",
      "weight:3.4413234141001836,bias:-0.32961164219216027\n",
      "Prediction:\n",
      "3.4413234141001836 -0.32961164219216027\n",
      "yhat: [-0.32961164 -0.32961164 -0.32961164 -0.32961164 -0.32961164 -0.32961164\n",
      " -0.32961164 -0.32961164  3.11171177 -0.32961164 -0.32961164  9.9943586\n",
      " -0.32961164 -0.32961164 -0.32961164 23.75965226 -0.32961164  6.55303519\n",
      " -0.32961164 -0.32961164 -0.32961164 -0.32961164 -0.32961164 -0.32961164\n",
      " -0.32961164 -0.32961164 16.87700543 -0.32961164 -0.32961164 -0.32961164\n",
      " -0.32961164 13.43568201 -0.32961164 -0.32961164 -0.32961164]\n"
     ]
    }
   ],
   "source": [
    "rms = RMSProp()\n",
    "history = rms.fit(x, y)\n",
    "print(\"Prediction:\")\n",
    "print(rms.w, rms.b)\n",
    "print(\"yhat:\", linear_regression(x, rms.w, rms.b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam\n",
    "class Adam:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.1\n",
    "        self.w = 9\n",
    "        self.b = 0\n",
    "        self.update_m_w = 0\n",
    "        self.update_m_b = 0\n",
    "        self.update_v_w = 0\n",
    "        self.update_v_b = 0\n",
    "        self.epsilon = 1e-8\n",
    "        self.beta1 = 0.9\n",
    "        self.beta2 = 0.99\n",
    "\n",
    "    def update_weight(self, x, y, e):\n",
    "        yhat = self.w * x + self.b\n",
    "        self.update_m_w = self.beta1 * self.update_m_w + (1 - self.beta1) * (\n",
    "            -2 * sum(x * (y - yhat)).mean()\n",
    "        )\n",
    "        self.update_m_b = self.beta1 * self.update_m_b + (1 - self.beta1) * (\n",
    "            -2 * sum(y - yhat).mean()\n",
    "        )\n",
    "        self.update_v_w = self.beta2 * self.update_v_w + (1 - self.beta2) * (\n",
    "            (-2 * sum(x * (y - yhat)).mean()) ** 2\n",
    "        )\n",
    "        self.update_v_b = self.beta2 * self.update_v_b + (1 - self.beta2) * (\n",
    "            (-2 * sum(y - yhat).mean()) ** 2\n",
    "        )\n",
    "        self.update_m_w = self.update_m_w / (1 - self.beta1**e + 1)\n",
    "        self.update_m_b = self.update_m_b / (1 - self.beta1**e + 1)\n",
    "        self.update_v_w = self.update_v_w / (1 - self.beta2**e + 1)\n",
    "        self.update_v_b = self.update_v_b / (1 - self.beta2**e + 1)\n",
    "        self.w -= (self.learning_rate / np.sqrt(self.update_v_w + self.epsilon)) * (\n",
    "            self.update_m_w\n",
    "        )\n",
    "        self.b -= (self.learning_rate / np.sqrt(self.update_v_b + self.epsilon)) * (\n",
    "            self.update_m_b\n",
    "        )\n",
    "\n",
    "    def MSE(self, y, yhat):\n",
    "        return np.square(np.subtract(y, yhat)).mean()\n",
    "\n",
    "    def fit(self, x, y, epochs=2000):\n",
    "        history = []\n",
    "        for e in range(epochs):\n",
    "            self.update_weight(x, y, e)\n",
    "            loss = self.MSE(y, (self.w * x + self.b))\n",
    "            if e % 100 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "            history.append(loss)\n",
    "            if loss <= 1:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "                return history\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 105.57314285714911\n",
      "weight:8.90000000000003,bias:-0.09999999999928146\n",
      "Epoch: 94, Loss: 0.9592518594789811\n",
      "weight:3.4706544458413977,bias:-0.29144372373459765\n",
      "Prediction:\n",
      "3.4706544458413977 -0.29144372373459765\n",
      "yhat: [-0.29144372 -0.29144372 -0.29144372 -0.29144372 -0.29144372 -0.29144372\n",
      " -0.29144372 -0.29144372  3.17921072 -0.29144372 -0.29144372 10.12051961\n",
      " -0.29144372 -0.29144372 -0.29144372 24.0031374  -0.29144372  6.64986517\n",
      " -0.29144372 -0.29144372 -0.29144372 -0.29144372 -0.29144372 -0.29144372\n",
      " -0.29144372 -0.29144372 17.06182851 -0.29144372 -0.29144372 -0.29144372\n",
      " -0.29144372 13.59117406 -0.29144372 -0.29144372 -0.29144372]\n"
     ]
    }
   ],
   "source": [
    "adam = Adam()\n",
    "history = adam.fit(x, y)\n",
    "print(\"Prediction:\")\n",
    "print(adam.w, adam.b)\n",
    "print(\"yhat:\", linear_regression(x, adam.w, adam.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. How does the \"Exponential weighted average\" lecture given in the readings   relate to some of the variants of Gradient Descent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight updates are guided by the previous gradients.The most recent gradient provide more information so the older the gradient the lesser the weight should be given thus exponential weightd average is used "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('study_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a24480c7c253f489f01188e56c2de50a3ffa81977a4b76b34cdd2b40b0691c29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
