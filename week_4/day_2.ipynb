{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "# day2: 30 Aug 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Learn about Gradient Descent and its below variants:\n",
    "\n",
    "    Momentum\n",
    "\n",
    "    Nesterov\n",
    "\n",
    "    Adagrad\n",
    "\n",
    "    RMSProp\n",
    "\n",
    "    Adam\n",
    "5. Implement all the above in Numpy\n",
    "\n",
    "[nestrov and adagrad added]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0, 3.0, 7.0, 2.0, 5.0, 4.0])\n",
    "y = np.array([4.0, 9.0, 20.0, 7.0, 15.0, 11.0])\n",
    "\n",
    "\n",
    "def linear_regression(x, m, b):\n",
    "    yhat = x * m + b\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient descent\n",
    "class GradientDescent:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.w = 9\n",
    "        self.b = 0\n",
    "\n",
    "    def update_weight(self, x, y):\n",
    "        yhat = self.w * x + self.b\n",
    "        self.w = self.w - self.learning_rate * (-2 * sum(x * (y - yhat)).mean())\n",
    "        self.b = self.b - self.learning_rate * (-2 * sum(y - yhat).mean())\n",
    "\n",
    "    def MSE(self, y, yhat):\n",
    "        return np.square(np.subtract(y, yhat)).mean()\n",
    "\n",
    "    def fit(self, x, y, epochs=200):\n",
    "        history = []\n",
    "        for e in range(epochs):\n",
    "            self.update_weight(x, y)\n",
    "            loss = self.MSE(y, (self.w * x + self.b))\n",
    "            if e % 10 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "            history.append(loss)\n",
    "            if loss <= 1:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "                return history\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 392.9781973333333\n",
      "weight:7.736,bias:-0.264\n",
      "Epoch: 10, Loss: 4.440984551944799\n",
      "weight:3.569367530187586,bias:-1.0900302710296121\n",
      "Epoch: 20, Loss: 1.4910552860496364\n",
      "weight:3.1983207104846607,bias:-1.1047724125864504\n",
      "Epoch: 30, Loss: 1.4063155896133022\n",
      "weight:3.15457713984864,bias:-1.0510573004841588\n",
      "Epoch: 40, Loss: 1.3460008693529024\n",
      "weight:3.1393310956949874,bias:-0.9928443907244823\n",
      "Epoch: 50, Loss: 1.2888678977593233\n",
      "weight:3.1268323692561566,bias:-0.9356082483828032\n",
      "Epoch: 60, Loss: 1.2346081693321485\n",
      "weight:3.1148551006538137,bias:-0.8797857821942265\n",
      "Epoch: 70, Loss: 1.1830761406341836\n",
      "weight:3.103200278433236,bias:-0.8253807833155423\n",
      "Epoch: 80, Loss: 1.1341346792547704\n",
      "weight:3.09184369185081,bias:-0.7723605909420802\n",
      "Epoch: 90, Loss: 1.0876535546911934\n",
      "weight:3.0807763694016863,bias:-0.7206902437229002\n",
      "Epoch: 100, Loss: 1.0435090833202458\n",
      "weight:3.069990827772471,bias:-0.6703354005758341\n",
      "Epoch: 110, Loss: 1.0015837992201375\n",
      "weight:3.059479882703988,bias:-0.6212625714824378\n",
      "Epoch: 111, Loss: 0.9975089814866424\n",
      "weight:3.058443620246786,bias:-0.616424535463624\n",
      "Prediction:\n",
      "yhat:[ 2.44201908  8.55890633 20.79268081  5.50046271 14.67579357 11.61734995]\n"
     ]
    }
   ],
   "source": [
    "gd = GradientDescent()\n",
    "gd.fit(x, y)\n",
    "print(\"Prediction:\")\n",
    "print(f\"yhat:{linear_regression(x,gd.w,gd.b)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum Accelerated Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Momentum\n",
    "class MomentumGradientDescent:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.01\n",
    "        self.momentum = 0.9\n",
    "        self.w = 9\n",
    "        self.b = 0\n",
    "        self.update_w = 0\n",
    "        self.update_b = 0\n",
    "\n",
    "    def update_weight(self, x, y):\n",
    "        yhat = self.w * x + self.b\n",
    "        self.update_w = self.momentum * self.update_w + self.learning_rate * (\n",
    "            -2 * sum(x * (y - yhat)).mean()\n",
    "        )\n",
    "        self.update_b = self.momentum * self.update_b + self.learning_rate * (\n",
    "            -2 * sum(y - yhat).mean()\n",
    "        )\n",
    "\n",
    "    def MSE(self, y, yhat):\n",
    "        return np.square(np.subtract(y, yhat)).mean()\n",
    "\n",
    "    def fit(self, x, y, epochs=200):\n",
    "        history = []\n",
    "        for e in range(epochs):\n",
    "            self.update_weight(x, y)\n",
    "            self.w -= self.update_w\n",
    "            self.b -= self.update_b\n",
    "            loss = self.MSE(y, (self.w * x + self.b))\n",
    "            if e % 10 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "            history.append(loss)\n",
    "            if loss <= 1:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "                return history\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 882.6997333333334\n",
      "weight:-3.6400000000000006,bias:-2.64\n",
      "Epoch: 10, Loss: 189.83126432585706\n",
      "weight:5.850688050169891,bias:1.7932931384780255\n",
      "Epoch: 20, Loss: 70.90183470769233\n",
      "weight:4.4046935820368285,bias:2.5186095375955104\n",
      "Epoch: 30, Loss: 36.11273870396934\n",
      "weight:1.268422195917521,bias:1.019642338552188\n",
      "Epoch: 36, Loss: 0.41546583455003255\n",
      "weight:2.8148830446765354,bias:1.0091843554380326\n",
      "Prediction:\n",
      "yhat: [ 3.8240674   9.45383349 20.71336567  6.63895044 15.08359958 12.26871653]\n"
     ]
    }
   ],
   "source": [
    "mgd = MomentumGradientDescent()\n",
    "mgd.fit(x, y)\n",
    "print(\"Prediction:\")\n",
    "print(\"yhat:\", linear_regression(x, mgd.w, mgd.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nestrov Accelerated Gradient Descent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nestrov\n",
    "class NestrovAcceleratedGradientDescent:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.001\n",
    "        self.momentum = 0.9\n",
    "        self.w = 9\n",
    "        self.b = 0\n",
    "        self.update_w = 0\n",
    "        self.update_b = 0\n",
    "        self.prev_w = 1\n",
    "        self.prev_b = 0\n",
    "\n",
    "    def update_weight(self, x, y):\n",
    "        self.update_w = self.momentum * self.prev_w\n",
    "        self.update_b = self.momentum * self.prev_b\n",
    "        yhat_look_ahead = (self.w - self.update_w) * x + (self.b - self.update_b)\n",
    "        self.update_w = self.momentum * self.prev_w + self.learning_rate * (\n",
    "            -2 * sum(x * (y - yhat_look_ahead)).mean()\n",
    "        )\n",
    "        self.update_b = self.momentum * self.prev_b + self.learning_rate * (\n",
    "            -2 * sum(y - yhat_look_ahead).mean()\n",
    "        )\n",
    "        self.w -= self.update_w\n",
    "        self.b -= self.update_b\n",
    "        self.prev_w = self.update_w\n",
    "        self.prev_b = self.update_b\n",
    "\n",
    "    def MSE(self, y, yhat):\n",
    "        return np.square(np.subtract(y, yhat)).mean()\n",
    "\n",
    "    def fit(self, x, y, epochs=200):\n",
    "        history = []\n",
    "        for e in range(epochs):\n",
    "            self.update_weight(x, y)\n",
    "\n",
    "            loss = self.MSE(y, (self.w * x + self.b))\n",
    "            history.append(loss)\n",
    "            if e % 10 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss})\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "            if loss <= 1:\n",
    "                print(f\"Epoch: {e}, Loss: {loss})\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "                return history\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 285.3847069333333)\n",
      "weight:7.0232,bias:-0.2244\n",
      "Epoch: 10, Loss: 7.521395494661022)\n",
      "weight:3.4862203458884564,bias:0.37097203124805433\n",
      "Epoch: 14, Loss: 0.9061846024676353)\n",
      "weight:2.986027250291028,bias:0.576608846920296\n",
      "2.986027250291028 0.576608846920296\n",
      "Prediction:\n",
      "yhat: [ 3.5626361   9.5346906  21.4787996   6.54866335 15.5067451  12.52071785]\n"
     ]
    }
   ],
   "source": [
    "ngd = NestrovAcceleratedGradientDescent()\n",
    "ngd.fit(x, y)\n",
    "print(ngd.w, ngd.b)\n",
    "print(\"Prediction:\")\n",
    "print(\"yhat:\", linear_regression(x, ngd.w, ngd.b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1.0,0.0,0.0, 3.0,0.0,0.0,0.0, 7.0,0.0, 2.0, 5.0,0.0,0.0,0.0,0.0, 4.0,0.0,0.0,0.0])\n",
    "y = np.array([4.0,0.0,-1.0, 9.0,0.0,0.0,0.0,20.0,0.0, 7.0, 15.0,0.0,0.1,0.0,0.0,11.0,0.0,1.0,0.0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adagrad\n",
    "class AdaGrad:\n",
    "    def __init__(self):\n",
    "        self.learning_rate = 0.5\n",
    "        self.w = 9\n",
    "        self.b = 0\n",
    "        self.update_w = 0\n",
    "        self.update_b = 0\n",
    "        self.epsilon = 1e-9\n",
    "\n",
    "    def update_weight(self, x, y):\n",
    "        yhat = self.w * x + self.b\n",
    "        self.update_w = self.update_w + (-2 * sum(x * (y - yhat)).mean()) ** 2\n",
    "        self.update_b = self.update_b + (-2 * sum(y - yhat).mean()) ** 2\n",
    "        self.w -= (self.learning_rate / np.sqrt(self.update_w + self.epsilon)) * (\n",
    "            -2 * sum(x * (y - yhat)).mean()\n",
    "        )\n",
    "        self.b -= (self.learning_rate / np.sqrt(self.update_b + self.epsilon)) * (\n",
    "            -2 * sum(y - yhat).mean()\n",
    "        )\n",
    "\n",
    "    def MSE(self, y, yhat):\n",
    "        return np.square(np.subtract(y, yhat)).mean()\n",
    "\n",
    "    def fit(self, x, y, epochs=2000):\n",
    "        history = []\n",
    "        for e in range(epochs):\n",
    "            self.update_weight(x, y)\n",
    "            loss = self.MSE(y, (self.w * x + self.b))\n",
    "            \n",
    "            history.append(loss)\n",
    "            if e % 100 == 0:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "            if loss <= 1:\n",
    "                print(f\"Epoch: {e}, Loss: {loss}\")\n",
    "                print(f\"weight:{self.w},bias:{self.b}\")\n",
    "                \n",
    "                return history\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 164.41368421052633\n",
      "weight:8.5,bias:-0.49999999999999645\n",
      "Epoch: 100, Loss: 2.8292549995701672\n",
      "weight:3.6307071174486465,bias:-1.299572613840077\n",
      "Epoch: 146, Loss: 0.9819553165685037\n",
      "weight:3.27670928643533,bias:-0.651028926261302\n",
      "Prediction:\n",
      "3.27670928643533 -0.651028926261302\n",
      "yhat: [ 2.62568036 -0.65102893 -0.65102893  9.17909893 -0.65102893 -0.65102893\n",
      " -0.65102893 22.28593608 -0.65102893  5.90238965 15.73251751 -0.65102893\n",
      " -0.65102893 -0.65102893 -0.65102893 12.45580822 -0.65102893 -0.65102893\n",
      " -0.65102893]\n"
     ]
    }
   ],
   "source": [
    "Agd = AdaGrad()\n",
    "Agd.fit(x, y)\n",
    "print(\"Prediction:\")\n",
    "print(Agd.w,Agd.b)\n",
    "print(\"yhat:\", linear_regression(x, Agd.w, Agd.b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('study_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a24480c7c253f489f01188e56c2de50a3ffa81977a4b76b34cdd2b40b0691c29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
